{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "988adf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "812fc0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching tweets: 403 Forbidden\n",
      "453 - You currently have access to a subset of Twitter API v2 endpoints and limited v1.1 endpoints (e.g. media post, oauth) only. If you need access to this endpoint, you may need a different access level. You can learn more here: https://developer.twitter.com/en/portal/product\n",
      "Fetched 0 tweets from BBCBreaking\n"
     ]
    }
   ],
   "source": [
    "# Twitter API credentials\n",
    "consumer_key = 'F5bwjzfNDf7LWlX0VbHeMqBQH'\n",
    "consumer_secret = 'aRv5Kxob0mta5tGbiW48cdxi8GdB12woo8bnIsNmMTgJ2Ma3ky'\n",
    "access_token = '1194164288132247552-zoWxVkGJ2MWfNW8HAkDIP6bPb6lpQC'\n",
    "access_token_secret = 'chFGTNtFh0Bdzpxr6TvXuZiHr4tjvj114TD3fFvpNQzLz'\n",
    "\n",
    "# Authenticate\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "# Create API object\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "\n",
    "def fetch_tweets(username, num_tweets=200):\n",
    "    tweets = []\n",
    "    try:\n",
    "        # Collect tweets\n",
    "        for tweet in tweepy.Cursor(api.user_timeline, screen_name=username, tweet_mode='extended').items(num_tweets):\n",
    "            tweets.append(tweet.full_text)\n",
    "    except tweepy.TweepyException as e:\n",
    "        print(f\"Error fetching tweets: {e}\")\n",
    "    \n",
    "    return tweets\n",
    "\n",
    "# Example usage:\n",
    "username = 'BBCBreaking'  # Replace with any Twitter username\n",
    "tweets = fetch_tweets(username, num_tweets=200)\n",
    "print(f\"Fetched {len(tweets)} tweets from {username}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8f8fdad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\velpu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\velpu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\velpu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim import corpora\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from dash import Dash, html, dcc, Input, Output\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6da39da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_tweets(username):\n",
    "    url = f\"https://x.com/{username}\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    tweets = [p.text for p in soup.find_all('p', class_='tweet-text')]\n",
    "    return tweets\n",
    "\n",
    "def fetch_news(api_key, query):\n",
    "    url = f\"https://newsapi.org/v2/everything?q={query}&apiKey={api_key}\"\n",
    "    response = requests.get(url).json()\n",
    "    articles = [article['content'] for article in response['articles'] if article['content']]\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6d97652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25f18c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_lda(docs, num_topics=10):\n",
    "    texts = [clean_text(doc).split() for doc in docs]\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    lda = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=15)\n",
    "    return lda, dictionary, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d439052d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 4: Generate Word Cloud\n",
    "def create_word_cloud(lda_model, dictionary):\n",
    "    topics = lda_model.show_topics(formatted=False)\n",
    "\n",
    "    for i, topic in topics:\n",
    "        topic_words = {\n",
    "        dictionary[word_id]: prob  # Mapping the word (from dictionary) to its probability\n",
    "        for word_id, prob in topic  # Loop through each (word_id, probability) pair in the topic\n",
    "            if word_id in dictionary.token2id.values()  # Only include word IDs that exist in the dictionary\n",
    "        }\n",
    "        if topic_words:\n",
    "            wordcloud = WordCloud(width=800, height=400).generate_from_frequencies(topic_words)\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.imshow(wordcloud, interpolation='bilinear')\n",
    "            plt.axis('off')\n",
    "            plt.title(f'Topic {i+1}')\n",
    "            plt.show()\n",
    "        \n",
    "\n",
    "# Step 5: Interactive Word Cloud with Dash\n",
    "def display_interactive_word_cloud(lda_model, dictionary, corpus, texts):\n",
    "    app = Dash(__name__)\n",
    "\n",
    "    # Extract topic data\n",
    "    topic_data = []\n",
    "    topics = lda_model.show_topics(formatted=False)\n",
    "    for i, topic in topics:\n",
    "        topic_words = {dictionary[word_id]: prob for word_id, prob in topic if word_id in dictionary.token2id.values()}\n",
    "        topic_data.append((i, topic_words))\n",
    "\n",
    "    def generate_word_cloud(topic_words):\n",
    "        wordcloud = WordCloud(width=800, height=400).generate_from_frequencies(topic_words)\n",
    "        fig = px.imshow(wordcloud.to_array())\n",
    "        fig.update_layout(coloraxis_showscale=False, title=\"Word Cloud\")\n",
    "        return fig\n",
    "\n",
    "    app.layout = html.Div([\n",
    "        dcc.Dropdown(\n",
    "            id='topic-dropdown',\n",
    "            options=[{'label': f'Topic {i+1}', 'value': i} for i, _ in enumerate(topics)],\n",
    "            value=0\n",
    "        ),\n",
    "        dcc.Graph(id='word-cloud'),\n",
    "        dcc.Graph(id='related-tweets')\n",
    "    ])\n",
    "\n",
    "    @app.callback(\n",
    "        Output('word-cloud', 'figure'),\n",
    "        Input('topic-dropdown', 'value')\n",
    "    )\n",
    "    def update_word_cloud(topic_index):\n",
    "        topic_words = topic_data[topic_index][1]\n",
    "        return generate_word_cloud(topic_words)\n",
    "\n",
    "    @app.callback(\n",
    "        Output('related-tweets', 'figure'),\n",
    "        Input('word-cloud', 'clickData'),\n",
    "        Input('topic-dropdown', 'value')\n",
    "    )\n",
    "    def display_related_tweets(click_data, topic_index):\n",
    "        if click_data:\n",
    "            word = click_data['points'][0]['label']\n",
    "            related_docs = [text for text, bow in zip(texts, corpus) if any(word in dictionary[id] for id, _ in bow)]\n",
    "            fig = go.Figure(data=[go.Table(\n",
    "                header=dict(values=['Related Tweets']),\n",
    "                cells=dict(values=[related_docs])\n",
    "            )])\n",
    "            return fig\n",
    "        return go.Figure()\n",
    "\n",
    "    app.run_server(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a897e5b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'requests' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     tweets \u001b[38;5;241m=\u001b[39m scrape_tweets(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtechnology\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(tweets)\n\u001b[0;32m      5\u001b[0m     api_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m9a2e8fd7507040aab3d87bb793f5a6cd\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m, in \u001b[0;36mscrape_tweets\u001b[1;34m(username)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscrape_tweets\u001b[39m(username):\n\u001b[0;32m      2\u001b[0m     url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://x.com/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00musername\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m     response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[0;32m      4\u001b[0m     soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m     tweets \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtweet-text\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'requests' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Example usage:\n",
    "    tweets = scrape_tweets('technology')\n",
    "    print(tweets)\n",
    "    api_key = '9a2e8fd7507040aab3d87bb793f5a6cd'\n",
    "    articles = fetch_news(api_key, 'technology')\n",
    "    print(articles)\n",
    "    lda_model, dictionary, corpus = apply_lda(articles)\n",
    "    create_word_cloud(lda_model, dictionary)\n",
    "    display_interactive_word_cloud(lda_model, dictionary, corpus, articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e8596f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
